============================================================
Job ID: 76423
Job Name: text_sft
Node: g0008
GPUs: 0
CPUs: 8
Start Time: Mon Jan  5 11:47:35 AM CST 2026
============================================================
==================================================
Physics-Informed SFT Training
==================================================
Using device: cuda
Training mode: text
Initializing model...

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
Loading checkpoint shards:  25%|Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Å’       | 1/4 [00:03<00:10,  3.51s/it]
Loading checkpoint shards:  50%|Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†     | 2/4 [00:07<00:07,  3.80s/it]
Loading checkpoint shards:  75%|Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Å’  | 3/4 [00:11<00:03,  3.83s/it]
Loading checkpoint shards: 100%|Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†| 4/4 [00:14<00:00,  3.48s/it]
Loading checkpoint shards: 100%|Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†| 4/4 [00:14<00:00,  3.58s/it]
/data/home/scyg723/RL-LNS/src/datalib/dataset.py:110: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.text_data = torch.load(self.data_path)
/data/home/scyg723/RL-LNS/src/datalib/dataset.py:116: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.graph_data = torch.load(graph_data_path)
wandb: Tracking run with wandb version 0.23.1
wandb: W&B syncing is set to `offline` in this directory. Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
wandb: Run data is saved locally in /data/home/scyg723/RL-LNS/wandb/offline-run-20260105_114808-j4l2w5b5
Loaded ./models/Qwen2.5-7B-Instruct with transformers (4-bit: True)
Loading text training data from data/processed/train_text.pt...
Loading text validation data from data/processed/val_text.pt...
Train samples: 10800
Val samples: 1199
Output directory: outputs/sft/text
Starting training...
Starting training:
  Num samples: 10800
  Num epochs: 3
  Batch size: 1
  Gradient accumulation: 16
  Effective batch size: 16
  Total steps: 937
  Trainable params: 164,234,754

Epoch 1/3

Epoch 1:   0%|          | 0/5000 [00:00<?, ?it/s]/data/home/scyg723/RL-LNS/src/model/text_tokenizer.py:318: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
/data/home/scyg723/.conda/envs/rl-lns/lib/python3.10/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(

Epoch 1:   0%|          | 0/5000 [05:46<?, ?it/s]
[DEBUG _extract_constraints] Keys in prepared: ['text', 'n_vars', 'n_constrs', 'target', 'A_row', 'A_col', 'A_val', 'A_shape', 'b', 'sense', 'var_types', 'mode', 'lp_relaxation', 'input_ids', 'attention_mask']
[DEBUG _extract_constraints] Built sparse A from COO components:
  A.dtype = torch.float32, A.shape = torch.Size([1676, 558]), A.is_sparse = True
  A_val.dtype = torch.float32
  var_types extracted: shape=torch.Size([558]), dtype=torch.int64
Traceback (most recent call last):
  File "/data/home/scyg723/RL-LNS/src/main.py", line 431, in <module>
    main()
  File "/data/home/scyg723/RL-LNS/src/main.py", line 419, in main
    train_sft(args)
  File "/data/home/scyg723/RL-LNS/src/main.py", line 222, in train_sft
    trainer.train()
  File "/data/home/scyg723/RL-LNS/src/training/sft_trainer.py", line 554, in train
    losses = self.train_step(batch)
  File "/data/home/scyg723/RL-LNS/src/training/sft_trainer.py", line 481, in train_step
    losses = self.loss_fn(
  File "/data/home/scyg723/.conda/envs/rl-lns/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/home/scyg723/.conda/envs/rl-lns/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/scyg723/RL-LNS/src/training/physics_loss.py", line 497, in forward
    l_task = self.task_loss(pred, target, var_types=var_types)
  File "/data/home/scyg723/.conda/envs/rl-lns/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/home/scyg723/.conda/envs/rl-lns/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/scyg723/RL-LNS/src/training/physics_loss.py", line 89, in forward
    binary_target = target[binary_mask]
IndexError: The shape of the mask [558] at index 0 does not match the shape of the indexed tensor [1, 558] at index 0
Traceback (most recent call last):
  File "/data/home/scyg723/RL-LNS/src/main.py", line 431, in <module>
    main()
  File "/data/home/scyg723/RL-LNS/src/main.py", line 419, in main
    train_sft(args)
  File "/data/home/scyg723/RL-LNS/src/main.py", line 222, in train_sft
    trainer.train()
  File "/data/home/scyg723/RL-LNS/src/training/sft_trainer.py", line 554, in train
    losses = self.train_step(batch)
  File "/data/home/scyg723/RL-LNS/src/training/sft_trainer.py", line 481, in train_step
    losses = self.loss_fn(
  File "/data/home/scyg723/.conda/envs/rl-lns/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/home/scyg723/.conda/envs/rl-lns/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/scyg723/RL-LNS/src/training/physics_loss.py", line 497, in forward
    l_task = self.task_loss(pred, target, var_types=var_types)
  File "/data/home/scyg723/.conda/envs/rl-lns/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/home/scyg723/.conda/envs/rl-lns/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/scyg723/RL-LNS/src/training/physics_loss.py", line 89, in forward
    binary_target = target[binary_mask]
IndexError: The shape of the mask [558] at index 0 does not match the shape of the indexed tensor [1, 558] at index 0
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /data/home/scyg723/RL-LNS/wandb/offline-run-20260105_114808-j4l2w5b5[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20260105_114808-j4l2w5b5/logs[0m
